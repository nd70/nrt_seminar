{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Addition Network\n",
    "================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "#matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 +  1 = 16\n",
      "15 + 15 = 30\n",
      " 3 + 22 = 25\n",
      " 7 +  1 =  8\n",
      " 7 +  7 = 14\n",
      "18 +  7 = 25\n",
      " 7 + 18 = 25\n",
      " 9 +  4 = 13\n",
      "20 +  4 = 24\n",
      "16 + 20 = 36\n"
     ]
    }
   ],
   "source": [
    "# make a list of 100 pairs from 0-25 for the network to add together\n",
    "input_data = np.random.randint(25, size=(100,2))\n",
    "target_data = np.array([x[0] + x[1] for x in input_data])\n",
    "for ii in range(10):\n",
    "    a, b = input_data[ii, :]\n",
    "    ans = target_data[ii]\n",
    "    print('{0:>2} + {1:>2} = {2:>2}'.format(a, b, ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Now that we have our data set, let's split it into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape=(75, 2), y_train.shape=(75,)\n",
      "x_test.shape=(25, 2), y_test.shape=(25,)\n"
     ]
    }
   ],
   "source": [
    "train_size = 75\n",
    "\n",
    "# this is the data we train the network on\n",
    "x_train = input_data[:train_size, :]\n",
    "y_train = target_data[:train_size]\n",
    "\n",
    "# this is the data that we test to see how well we did during training\n",
    "x_test = input_data[train_size:, :]\n",
    "y_test = target_data[train_size:]\n",
    "\n",
    "print('x_train.shape={0}, y_train.shape={1}'.format(x_train.shape, y_train.shape))\n",
    "print('x_test.shape={0}, y_test.shape={1}'.format(x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Network Architecture\n",
    "====================\n",
    "\n",
    "Now that we have our data split the way we want it, we can design the architecture of the network.\n",
    "\n",
    "Since the task at hand is linear, we will first use linear activation functions.\n",
    "\n",
    "To keep things simple, we will use the Mean Square Error loss function and the Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 75 samples, validate on 25 samples\n",
      "Epoch 1/100\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 53.8786 - val_loss: 28.4418\n",
      "Epoch 2/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 15.8106 - val_loss: 10.9260\n",
      "Epoch 3/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 7.0144 - val_loss: 7.5543\n",
      "Epoch 4/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 5.2460 - val_loss: 6.1277\n",
      "Epoch 5/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 4.3506 - val_loss: 5.0097\n",
      "Epoch 6/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 3.5760 - val_loss: 4.0416\n",
      "Epoch 7/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 2.8181 - val_loss: 3.1809\n",
      "Epoch 8/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 2.1953 - val_loss: 2.4752\n",
      "Epoch 9/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.6965 - val_loss: 1.8656\n",
      "Epoch 10/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.2843 - val_loss: 1.3759\n",
      "Epoch 11/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.9444 - val_loss: 0.9946\n",
      "Epoch 12/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.6762 - val_loss: 0.6966\n",
      "Epoch 13/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4784 - val_loss: 0.4986\n",
      "Epoch 14/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3365 - val_loss: 0.3356\n",
      "Epoch 15/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2294 - val_loss: 0.2211\n",
      "Epoch 16/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1562 - val_loss: 0.1469\n",
      "Epoch 17/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1057 - val_loss: 0.0972\n",
      "Epoch 18/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0725 - val_loss: 0.0638\n",
      "Epoch 19/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0509 - val_loss: 0.0416\n",
      "Epoch 20/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0363 - val_loss: 0.0290\n",
      "Epoch 21/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0271 - val_loss: 0.0220\n",
      "Epoch 22/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0216 - val_loss: 0.0169\n",
      "Epoch 23/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0185 - val_loss: 0.0141\n",
      "Epoch 24/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0165 - val_loss: 0.0127\n",
      "Epoch 25/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0146 - val_loss: 0.0120\n",
      "Epoch 26/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0138 - val_loss: 0.0106\n",
      "Epoch 27/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0128 - val_loss: 0.0101\n",
      "Epoch 28/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0121 - val_loss: 0.0096\n",
      "Epoch 29/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0114 - val_loss: 0.0091\n",
      "Epoch 30/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0111 - val_loss: 0.0094\n",
      "Epoch 31/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0097 - val_loss: 0.0084\n",
      "Epoch 32/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0095 - val_loss: 0.0077\n",
      "Epoch 33/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0093 - val_loss: 0.0071\n",
      "Epoch 34/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.0074\n",
      "Epoch 35/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0079 - val_loss: 0.0061\n",
      "Epoch 36/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0072 - val_loss: 0.0058\n",
      "Epoch 37/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0067 - val_loss: 0.0053\n",
      "Epoch 38/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0063 - val_loss: 0.0049\n",
      "Epoch 39/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 0.0048\n",
      "Epoch 40/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 0.0042\n",
      "Epoch 41/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0049 - val_loss: 0.0037\n",
      "Epoch 42/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0047 - val_loss: 0.0034\n",
      "Epoch 43/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.0031\n",
      "Epoch 44/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.0034\n",
      "Epoch 45/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0037 - val_loss: 0.0025\n",
      "Epoch 46/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0029 - val_loss: 0.0024\n",
      "Epoch 47/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0028 - val_loss: 0.0021\n",
      "Epoch 48/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0025 - val_loss: 0.0019\n",
      "Epoch 49/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0023 - val_loss: 0.0015\n",
      "Epoch 50/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 51/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0016 - val_loss: 0.0012\n",
      "Epoch 52/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 53/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0012 - val_loss: 9.3022e-04\n",
      "Epoch 54/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0011 - val_loss: 8.7540e-04\n",
      "Epoch 55/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 8.9887e-04 - val_loss: 6.7339e-04\n",
      "Epoch 56/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.6185e-04 - val_loss: 5.6760e-04\n",
      "Epoch 57/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.8413e-04 - val_loss: 5.5659e-04\n",
      "Epoch 58/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.8013e-04 - val_loss: 3.8826e-04\n",
      "Epoch 59/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.1776e-04 - val_loss: 3.9549e-04\n",
      "Epoch 60/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 3.9194e-04 - val_loss: 2.8208e-04\n",
      "Epoch 61/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 3.5656e-04 - val_loss: 2.5364e-04\n",
      "Epoch 62/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 2.7849e-04 - val_loss: 1.8066e-04\n",
      "Epoch 63/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 2.2474e-04 - val_loss: 1.5481e-04\n",
      "Epoch 64/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.7459e-04 - val_loss: 1.2459e-04\n",
      "Epoch 65/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3174e-04 - val_loss: 1.0602e-04\n",
      "Epoch 66/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 9.7894e-05 - val_loss: 8.5644e-05\n",
      "Epoch 67/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 8.9808e-05 - val_loss: 6.3465e-05\n",
      "Epoch 68/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.0447e-05 - val_loss: 5.1676e-05\n",
      "Epoch 69/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 5.1303e-05 - val_loss: 4.1915e-05\n",
      "Epoch 70/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 3.9410e-05 - val_loss: 2.6687e-05\n",
      "Epoch 71/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 3.7182e-05 - val_loss: 1.8887e-05\n",
      "Epoch 72/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 2.1880e-05 - val_loss: 1.6590e-05\n",
      "Epoch 73/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.7017e-05 - val_loss: 1.1007e-05\n",
      "Epoch 74/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3194e-05 - val_loss: 7.3764e-06\n",
      "Epoch 75/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.0313e-05 - val_loss: 6.3920e-06\n",
      "Epoch 76/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.4475e-06 - val_loss: 4.2631e-06\n",
      "Epoch 77/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 4.5231e-06 - val_loss: 3.8481e-06\n",
      "Epoch 78/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 4.6315e-06 - val_loss: 1.8811e-06\n",
      "Epoch 79/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 2.0500e-06 - val_loss: 1.3414e-06\n",
      "Epoch 80/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.9899e-06 - val_loss: 7.8360e-07\n",
      "Epoch 81/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 9.6009e-07 - val_loss: 5.1292e-07\n",
      "Epoch 82/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.7480e-07 - val_loss: 3.2031e-07\n",
      "Epoch 83/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 3.5281e-07 - val_loss: 2.6423e-07\n",
      "Epoch 84/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 2.3282e-07 - val_loss: 1.5495e-07\n",
      "Epoch 85/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.6108e-07 - val_loss: 9.0415e-08\n",
      "Epoch 86/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.0747e-07 - val_loss: 8.8737e-08\n",
      "Epoch 87/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 7.6375e-08 - val_loss: 3.0914e-08\n",
      "Epoch 88/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 3.2472e-08 - val_loss: 2.5105e-08\n",
      "Epoch 89/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 2.4736e-08 - val_loss: 1.0148e-08\n",
      "Epoch 90/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.2005e-08 - val_loss: 5.7124e-09\n",
      "Epoch 91/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 6.5885e-09 - val_loss: 4.6097e-09\n",
      "Epoch 92/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 2.8086e-09 - val_loss: 2.6105e-09\n",
      "Epoch 93/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.8834e-09 - val_loss: 8.6119e-10\n",
      "Epoch 94/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.7975e-09 - val_loss: 5.1180e-10\n",
      "Epoch 95/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 4.7923e-10 - val_loss: 2.7417e-10\n",
      "Epoch 96/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 2.0941e-10 - val_loss: 3.9761e-10\n",
      "Epoch 97/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.8044e-10 - val_loss: 5.8299e-11\n",
      "Epoch 98/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 3.8011e-11 - val_loss: 1.6280e-11\n",
      "Epoch 99/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4422e-11 - val_loss: 5.7207e-12\n",
      "Epoch 100/100\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 6.9303e-12 - val_loss: 5.8662e-12\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(2, input_dim=x_train.shape[1], activation='linear'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "history = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test), batch_size=1)\n",
    "y_hat = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's log the loss functions and plot them \n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss History')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.show('addition_loss.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The loss looks great. There is no reason to believe that we are overfitting so the network weights should be well trained for this purpose. Let's look at the actual results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 + 14 = 25.9999980927\n",
      "23 +  9 = 31.9999980927\n",
      "21 +  0 = 21.0      \n",
      " 5 + 17 = 21.9999980927\n",
      "18 +  5 = 23.0000019073\n",
      " 2 + 17 = 19.0000019073\n",
      "14 + 19 = 32.9999961853\n",
      "11 + 17 = 27.9999980927\n",
      " 1 + 19 = 19.9999980927\n",
      " 7 +  4 = 11.0000038147\n",
      "23 +  7 = 29.9999980927\n",
      "12 + 22 = 34.0      \n",
      "12 +  6 = 18.0000019073\n",
      " 9 + 12 = 21.0000019073\n",
      " 4 + 22 = 25.9999961853\n",
      " 4 +  2 = 6.00000476837\n",
      "15 + 18 = 33.0      \n",
      " 6 +  1 = 7.00000476837\n",
      "24 + 13 = 37.0      \n",
      "16 + 24 = 40.0000038147\n",
      "16 +  8 = 24.0      \n",
      "19 + 13 = 32.0000038147\n",
      " 6 + 18 = 23.9999980927\n",
      " 1 + 19 = 19.9999980927\n",
      " 3 +  5 = 8.0000038147\n"
     ]
    }
   ],
   "source": [
    "for ii in range(x_test.shape[0]):\n",
    "    a, b = x_test[ii, :]\n",
    "    true_ans = y_test[ii]\n",
    "    network_ans = y_hat[ii][0]\n",
    "    print('{0:>2} + {1:>2} = {2:<10}'.format(a,b,network_ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "And there we have it! To >= 5 decimal places, we have taught a neural network how to add 2 numbers together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[-1.0307773 , -0.8174403 ],\n",
      "       [ 1.0303065 ,  0.31815538]], dtype=float32), array([0.00052368, 0.00132442], dtype=float32), '\\n')\n",
      "(array([[-1.3847893],\n",
      "       [-0.4148374]], dtype=float32), array([0.00127966], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "weights0, biases0 = model.layers[0].get_weights()\n",
    "weights1, biases1 = model.layers[1].get_weights()\n",
    "\n",
    "print(weights0.T, biases0, '\\n')\n",
    "print(weights1, biases1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "From this, we can see that the network has chosen a different set of weights than we had. Specifically, the network is doing the following operation to the input vector (a,b) (ignoring the biases which are close to 0)\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{pmatrix}\n",
    "    -1.3847893 & -0.4148374\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "    -1.0307773 & -0.8174403\\\\\n",
    "    1.0303065 &  0.31815538\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "    a\\\\\n",
    "    b\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "0.9999999 & 0.9999998\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "    a\\\\\n",
    "    b\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9999999 0.9999998]]\n"
     ]
    }
   ],
   "source": [
    "tot_weights = weights1.T.dot(weights0.T)\n",
    "print(tot_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "This is the result of multiplying the weights. When we dot the input with this, we can see that we very nearly get the sum of the input vector.\n",
    "\n",
    "To see another network which performs symbolic addition on a string (e.g, input \"12+16\" and the network says 38) look [here](https://github.com/keras-team/keras/blob/4f2e65c385d60fa87bb143c6c506cbe428895f44/examples/addition_rnn.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepclean",
   "language": "python",
   "name": "deepclean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
